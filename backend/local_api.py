import os
import torch
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM
import uvicorn

# å°è¯•å¯¼å…¥GLM4Vç‰¹å®šçš„æ¨¡å‹ç±»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    from transformers import Glm4vForConditionalGeneration
    HAS_GLM4V = True
except ImportError:
    try:
        # å°è¯•ä»glm4væ¨¡å—å¯¼å…¥
        from transformers.models.glm4v import Glm4vForConditionalGeneration
        HAS_GLM4V = True
    except ImportError:
        HAS_GLM4V = False
        print("âš ï¸  è­¦å‘Š: æ— æ³•å¯¼å…¥ Glm4vForConditionalGeneration")
        print("âš ï¸  æç¤º: å¯èƒ½éœ€è¦æ›´æ–°transformersç‰ˆæœ¬: pip install transformers>=5.0.0rc0")


# ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå¦åˆ™å°è¯•æœ¬åœ°è·¯å¾„ï¼Œæœ€åä½¿ç”¨Hugging Face
_default_model = "zai-org/AutoGLM-Phone-9B"
_local_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "models", "AutoGLM-Phone-9B")

if os.getenv("LOCAL_MODEL_ID"):
    MODEL_ID = os.getenv("LOCAL_MODEL_ID")
elif os.path.exists(_local_path) and os.path.isdir(_local_path):
    MODEL_ID = _local_path
    print(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {MODEL_ID}")
else:
    MODEL_ID = _default_model
    print(f"ä½¿ç”¨Hugging Faceæ¨¡å‹: {MODEL_ID}")
app = FastAPI(title="Local Transformers API")


class Message(BaseModel):
    role: str
    content: str


class ChatRequest(BaseModel):
    model: str
    messages: list[Message]
    max_tokens: int | None = 512
    temperature: float | None = 0.7
    top_p: float | None = 0.9


def _load_model():
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_ID}")
    use_mps = torch.backends.mps.is_available()
    device = "mps" if use_mps else "cpu"
    dtype = torch.float16 if use_mps else torch.float32
    
    print(f"ä½¿ç”¨è®¾å¤‡: {device}, æ•°æ®ç±»å‹: {dtype}")

    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_ID, 
        trust_remote_code=True,
        fix_mistral_regex=True  # ä¿®å¤tokenizerè­¦å‘Š
    )
    
    # GLM4Væ˜¯å¤šæ¨¡æ€æ¨¡å‹ï¼Œéœ€è¦å°è¯•ä½¿ç”¨Glm4vForConditionalGeneration
    # æ³¨æ„ï¼šGLM4Væ¨¡å‹é€šå¸¸éœ€è¦é€šè¿‡vLLMç­‰æœåŠ¡è¿è¡Œï¼Œç›´æ¥åŠ è½½å¯èƒ½åŠŸèƒ½å—é™
    try:
        # æ–¹æ³•1: å°è¯•ä½¿ç”¨Glm4vForConditionalGenerationï¼ˆå¦‚æœæœ‰ï¼‰
        if HAS_GLM4V:
            print("å°è¯•ä½¿ç”¨ Glm4vForConditionalGeneration åŠ è½½...")
            try:
                model = Glm4vForConditionalGeneration.from_pretrained(
                    MODEL_ID,
                    dtype=dtype,
                    device_map="auto",
                    trust_remote_code=True,
                )
                print(f"âœ“ æˆåŠŸä½¿ç”¨ Glm4vForConditionalGeneration åŠ è½½")
                if hasattr(model, 'generate'):
                    print("âœ“ æ¨¡å‹æ”¯æŒgenerateæ–¹æ³•")
                else:
                    print("âš ï¸  è­¦å‘Š: æ¨¡å‹æ²¡æœ‰generateæ–¹æ³•")
            except Exception as e:
                print(f"âš ï¸  Glm4vForConditionalGenerationåŠ è½½å¤±è´¥: {e}")
                print("å°è¯•ä½¿ç”¨ AutoModel åŠ è½½...")
                model = AutoModel.from_pretrained(
                    MODEL_ID,
                    dtype=dtype,
                    device_map="auto",
                    trust_remote_code=True,
                )
        else:
            # æ–¹æ³•2: ä½¿ç”¨AutoModelï¼ˆä¼šåŠ è½½ä¸ºGlm4vModelï¼Œæ²¡æœ‰generateæ–¹æ³•ï¼‰
            print("GLM4Væ˜¯å¤šæ¨¡æ€æ¨¡å‹ï¼Œä½¿ç”¨ AutoModel åŠ è½½...")
            print("âš ï¸  æ³¨æ„: AutoModelä¼šåŠ è½½ä¸ºGlm4vModelï¼Œå¯èƒ½æ²¡æœ‰generateæ–¹æ³•")
            model = AutoModel.from_pretrained(
                MODEL_ID,
                dtype=dtype,
                device_map="auto",
                trust_remote_code=True,
            )
        
        # æ£€æŸ¥æ¨¡å‹ç±»å‹å’ŒåŠŸèƒ½
        model_type = type(model).__name__
        print(f"æ¨¡å‹ç±»å‹: {model_type}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰generateæ–¹æ³•
        if not hasattr(model, 'generate'):
            print("âš ï¸  è­¦å‘Š: æ¨¡å‹æ²¡æœ‰generateæ–¹æ³•")
            print("âš ï¸  æç¤º: GLM4Væ¨¡å‹å»ºè®®ä½¿ç”¨vLLMæœåŠ¡ä»¥è·å¾—å®Œæ•´åŠŸèƒ½")
            print("âš ï¸  å½“å‰å®ç°å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œï¼Œå»ºè®®ä½¿ç”¨vLLMæœåŠ¡")
            print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ: è¿è¡Œ ./start_vllm.sh å¯åŠ¨vLLMæœåŠ¡")
            
    except ValueError as e:
        if "Unrecognized configuration class" in str(e) or "Glm4vConfig" in str(e):
            print("âŒ é”™è¯¯: GLM4Væ¨¡å‹æ— æ³•ç›´æ¥ç”¨transformersåŠ è½½")
            print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            print("   1. ä½¿ç”¨vLLMæœåŠ¡ï¼ˆæ¨èï¼‰:")
            print("      python3 -m vllm.entrypoints.openai.api_server \\")
            print("        --model ./models/AutoGLM-Phone-9B \\")
            print("        --port 8000")
            print("   2. æˆ–å‚è€ƒREADME.mdä¸­çš„vLLMéƒ¨ç½²æŒ‡å—")
            raise ValueError(
                "GLM4Væ¨¡å‹éœ€è¦ä½¿ç”¨vLLMæœåŠ¡ï¼Œä¸èƒ½ç›´æ¥ç”¨transformersåŠ è½½ã€‚"
                "è¯·ä½¿ç”¨vLLMå¯åŠ¨æ¨¡å‹æœåŠ¡ã€‚"
            ) from e
        else:
            raise
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
        print("ğŸ’¡ æç¤º: GLM4Væ¨¡å‹å»ºè®®ä½¿ç”¨vLLMæœåŠ¡")
        raise
    
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    print("æ¨¡å‹åŠ è½½å®Œæˆ!")
    return tokenizer, model, device


# å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
print("=" * 50)
print("åˆå§‹åŒ–æ¨¡å‹æœåŠ¡...")
tokenizer, model, device = _load_model()
print("=" * 50)


@app.post("/v1/chat/completions")
def chat(req: ChatRequest):
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰generateæ–¹æ³•
    if not hasattr(model, 'generate'):
        return {
            "error": {
                "message": "æ¨¡å‹ä¸æ”¯æŒæ–‡æœ¬ç”Ÿæˆã€‚GLM4Væ¨¡å‹éœ€è¦ä½¿ç”¨vLLMæœåŠ¡ã€‚",
                "type": "unsupported_model",
                "solution": "è¯·ä½¿ç”¨ ./start_vllm.sh å¯åŠ¨vLLMæœåŠ¡ï¼Œæˆ–å‚è€ƒ vLLMå¯åŠ¨æŒ‡å—.md"
            }
        }
    
    # æ„å»ºå¯¹è¯æ ¼å¼
    prompt = "\n".join([f"{m.role}: {m.content}" for m in req.messages]) + "\nassistant:"
    
    try:
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # ä½¿ç”¨generateæ–¹æ³•ç”Ÿæˆæ–‡æœ¬
        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=req.max_tokens or 512,
                temperature=req.temperature or 0.7,
                top_p=req.top_p or 0.9,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
            )
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        generated_text = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        
        return {
            "id": "chatcmpl-local",
            "object": "chat.completion",
            "model": req.model,
            "choices": [
                {
                    "index": 0,
                    "message": {"role": "assistant", "content": generated_text},
                    "finish_reason": "stop",
                }
            ],
        }
    except Exception as e:
        return {
            "error": {
                "message": f"ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}",
                "type": "generation_error",
                "solution": "GLM4Væ¨¡å‹å»ºè®®ä½¿ç”¨vLLMæœåŠ¡ä»¥è·å¾—å®Œæ•´åŠŸèƒ½"
            }
        }
    return {
        "id": "chatcmpl-local",
        "object": "chat.completion",
        "model": req.model,
        "choices": [
            {
                "index": 0,
                "message": {"role": "assistant", "content": text},
                "finish_reason": "stop",
            }
        ],
    }


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

