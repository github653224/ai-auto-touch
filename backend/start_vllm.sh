#!/bin/bash

# vLLMæ¨¡å‹æœåŠ¡å¯åŠ¨è„šæœ¬ï¼ˆæ¨èæ–¹æ¡ˆï¼‰
# ä½¿ç”¨æ–¹æ³•: ./start_vllm.sh æˆ– bash start_vllm.sh
# å‰æ: å·²åˆ›å»ºå¹¶æ¿€æ´»condaç¯å¢ƒ ai-auto-touchï¼Œä¸”å·²å®‰è£…vLLM

echo "=========================================="
echo "vLLMæ¨¡å‹æœåŠ¡å¯åŠ¨è„šæœ¬ (æ¨è)"
echo "=========================================="

# Condaç¯å¢ƒåç§°
CONDA_ENV_NAME="ai-auto-touch"

# åˆå§‹åŒ–conda
echo "åˆå§‹åŒ–conda..."
eval "$(conda shell.bash hook)"

# æ¿€æ´»condaç¯å¢ƒ
echo "æ¿€æ´»condaç¯å¢ƒ: ${CONDA_ENV_NAME}..."
if conda activate ${CONDA_ENV_NAME}; then
    echo "âœ“ condaç¯å¢ƒå·²æ¿€æ´»"
else
    echo "é”™è¯¯: æ— æ³•æ¿€æ´»condaç¯å¢ƒ ${CONDA_ENV_NAME}"
    echo "è¯·å…ˆåˆ›å»ºcondaç¯å¢ƒ: conda create -n ${CONDA_ENV_NAME} python=3.10"
    exit 1
fi

# æ£€æŸ¥Pythonç‰ˆæœ¬
echo "æ£€æŸ¥Pythonç¯å¢ƒ..."
python_version=$(python --version 2>&1)
echo "Pythonç‰ˆæœ¬: $python_version"

# æ£€æŸ¥vLLMæ˜¯å¦å·²å®‰è£…
echo "æ£€æŸ¥vLLMä¾èµ–..."
if ! python -c "import vllm" 2>/dev/null; then
    echo "vLLMæœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…..."
    echo "æç¤º: vLLMå®‰è£…å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…"
    pip install vllm>=0.12.0 transformers>=5.0.0rc0
    if [ $? -ne 0 ]; then
        echo "âŒ vLLMå®‰è£…å¤±è´¥"
        echo "ğŸ’¡ æç¤º: å¦‚æœå®‰è£…å¤±è´¥ï¼Œå¯èƒ½éœ€è¦å…ˆå®‰è£…CUDAæˆ–å…¶ä»–ä¾èµ–"
        exit 1
    fi
else
    vllm_version=$(python -c "import vllm; print(vllm.__version__)" 2>/dev/null)
    echo "âœ“ vLLMå·²å®‰è£…: $vllm_version"
fi

# è·å–æ¨¡å‹è·¯å¾„
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
MODEL_DIR="${PROJECT_ROOT}/models/AutoGLM-Phone-9B"

# æ£€æŸ¥æ¨¡å‹è·¯å¾„
if [ -d "${MODEL_DIR}" ]; then
    echo "âœ“ æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹: ${MODEL_DIR}"
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´
    if [ -f "${MODEL_DIR}/config.json" ] && [ -f "${MODEL_DIR}/model.safetensors.index.json" ]; then
        echo "âœ“ æ¨¡å‹æ–‡ä»¶å®Œæ•´"
        MODEL_PATH="${MODEL_DIR}"
    else
        echo "è­¦å‘Š: æ¨¡å‹ç›®å½•å­˜åœ¨ä½†æ–‡ä»¶ä¸å®Œæ•´"
        echo "å°†ä½¿ç”¨Hugging Faceæ¨¡å‹: zai-org/AutoGLM-Phone-9B"
        MODEL_PATH="zai-org/AutoGLM-Phone-9B"
    fi
else
    echo "æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ç›®å½•: ${MODEL_DIR}"
    echo "å°†ä½¿ç”¨Hugging Faceæ¨¡å‹: zai-org/AutoGLM-Phone-9B"
    echo "æç¤º: é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´"
    MODEL_PATH="zai-org/AutoGLM-Phone-9B"
fi

# æ£€æŸ¥ç«¯å£
if lsof -Pi :8000 -sTCP:LISTEN -t >/dev/null 2>&1 ; then
    echo "è­¦å‘Š: ç«¯å£8000å·²è¢«å ç”¨"
    echo "è¯·å…ˆåœæ­¢å ç”¨8000ç«¯å£çš„æœåŠ¡ï¼Œæˆ–ä¿®æ”¹å¯åŠ¨å‘½ä»¤ä¸­çš„ç«¯å£"
    exit 1
fi

# å¯åŠ¨æœåŠ¡
echo ""
echo "=========================================="
echo "å¯åŠ¨vLLMæ¨¡å‹æœåŠ¡..."
echo "æœåŠ¡åœ°å€: http://localhost:8000"
echo "APIç«¯ç‚¹: http://localhost:8000/v1/chat/completions"
echo "æ¨¡å‹è·¯å¾„: ${MODEL_PATH}"
echo "æŒ‰ Ctrl+C åœæ­¢æœåŠ¡"
echo "=========================================="
echo ""

# æ„å»ºvLLMå¯åŠ¨å‘½ä»¤
VLLM_CMD="python3 -m vllm.entrypoints.openai.api_server"
VLLM_CMD="${VLLM_CMD} --served-model-name autoglm-phone-9b"
VLLM_CMD="${VLLM_CMD} --allowed-local-media-path /"
VLLM_CMD="${VLLM_CMD} --mm-encoder-tp-mode data"
VLLM_CMD="${VLLM_CMD} --mm_processor_cache_type shm"
VLLM_CMD="${VLLM_CMD} --mm_processor_kwargs '{\"max_pixels\":5000000}'"
VLLM_CMD="${VLLM_CMD} --max-model-len 25480"
VLLM_CMD="${VLLM_CMD} --chat-template-content-format string"
VLLM_CMD="${VLLM_CMD} --limit-mm-per-prompt '{\"image\":10}'"
VLLM_CMD="${VLLM_CMD} --model ${MODEL_PATH}"
VLLM_CMD="${VLLM_CMD} --port 8000"

echo "æ‰§è¡Œå‘½ä»¤:"
echo "${VLLM_CMD}"
echo ""

# æ‰§è¡Œå¯åŠ¨å‘½ä»¤
eval ${VLLM_CMD}

