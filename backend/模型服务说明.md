# AI模型服务说明

项目需要AI模型服务来提供自然语言理解能力。有两种启动方案：

## 方案对比

| 特性 | vLLM (推荐) | local_api.py (简单) |
|------|------------|-------------------|
| **性能** | ⭐⭐⭐⭐⭐ 高性能，支持批量推理 | ⭐⭐⭐ 单请求处理 |
| **内存占用** | 较高 | 中等 |
| **启动速度** | 快 | 慢（首次需下载模型） |
| **适用场景** | 生产环境、多设备并发 | 本地测试、开发调试 |
| **配置复杂度** | 较高 | 低 |
| **端口** | 8000 | 8000 |

## 方案一：使用 vLLM（推荐，性能更好）

### 快速启动（推荐）

使用启动脚本：

```bash
cd backend
./start_vllm.sh
```

### 手动启动

1. **安装vLLM依赖**（在conda环境中）：
```bash
conda activate ai-auto-touch
pip install vllm>=0.12.0 transformers>=5.0.0rc0
```

2. **启动vLLM服务**：
```bash
python3 -m vllm.entrypoints.openai.api_server \
  --served-model-name autoglm-phone-9b \
  --allowed-local-media-path / \
  --mm-encoder-tp-mode data \
  --mm_processor_cache_type shm \
  --mm_processor_kwargs "{\"max_pixels\":5000000}" \
  --max-model-len 25480 \
  --chat-template-content-format string \
  --limit-mm-per-prompt "{\"image\":10}" \
  --model ./models/AutoGLM-Phone-9B \
  --port 8000
```

详细说明请参考：
- `vLLM启动指南.md` - 详细的vLLM使用说明
- `README.md` 的 1.3.3 节 - 完整的部署指南

## 方案二：使用 local_api.py（简单，适合本地测试）

### 启动步骤

```bash
cd backend
./start_local_model.sh
```

或手动启动：

```bash
cd backend
conda activate ai-auto-touch
python local_api.py
```

### 特点

- ✅ 使用 transformers 直接加载模型，无需额外配置
- ✅ 自动检测并使用本地模型（如果存在 `../models/AutoGLM-Phone-9B`）
- ✅ 首次运行会自动从 Hugging Face 下载模型
- ⚠️ 性能较 vLLM 低，适合单设备测试

### 环境变量

可以通过环境变量指定模型路径：

```bash
export LOCAL_MODEL_ID="../models/AutoGLM-Phone-9B"
python local_api.py
```

## 验证服务

无论使用哪种方案，启动后都可以通过以下方式验证：

```bash
# 检查服务是否运行
curl http://localhost:8000/v1/models

# 或访问API文档（如果支持）
# 浏览器打开: http://localhost:8000/docs
```

## 后端服务配置

后端服务（`main.py`）会自动连接到 `http://localhost:8000/v1`，无需修改配置。

如果需要修改，可以在 `.env` 文件中设置：

```bash
AUTOGLM_BASE_URL=http://localhost:8000/v1
```

## 常见问题

### 1. 端口冲突

如果8000端口被占用：

- **vLLM**: 修改启动命令中的 `--port` 参数
- **local_api.py**: 修改文件中的 `port=8000` 为其他端口
- 同时修改后端 `.env` 中的 `AUTOGLM_BASE_URL`

### 2. 模型下载慢

- 使用 ModelScope（国内加速）
- 或手动下载模型到 `./models/AutoGLM-Phone-9B`

### 3. 内存不足

- 使用 `local_api.py`（内存占用更小）
- 或减少 vLLM 的 `--max-model-len` 参数

## 启动顺序

完整的启动流程：

1. **启动AI模型服务**（二选一）：
   - 方案1: 启动 vLLM 服务
   - 方案2: 运行 `./start_local_model.sh`

2. **启动后端服务**：
   ```bash
   cd backend
   ./start_conda.sh
   ```

3. **启动前端服务**（可选）：
   ```bash
   cd frontend
   npm install
   npm run dev
   ```

